<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | ECE, Virginia Tech | Fall 2015: ECE 5554/4984</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
p{
	text-indent:2.0em;
	font-size:15px;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body leftmargin='100' rightmargin='100'>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Specialized OCR on Detecting Car Plates (Final Update 12/03/2019)</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Jesse Huang
        (jhuang347), Shuyi Teng (steng6)</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2019 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span><br>
<span style="font-size: 18px; line-height: 1.5em;">
    <a href='https://github.com/shuyteng/CS6476-Project'>Github Repo</a><br>
    <a href='update_1.html'>Midterm Update</a><br>
    <a href='proposal.html'>Original Proposal</a><br></span>
<hr>


<!-- Abstract -->
<h3>Abstract</h3>
<p>
	An OCR system specialized for detecting license plates in real-life images,
    with the ability to differentiate between car plates and other texted
    objects like road signs and shop signs. We will experiment with k-means
    clustering, graph cut methods (and more) to extract plate images and use
    different OCR systems to recognize plate information.
</p>
<br>

<!-- Introduction -->
<h3>Introduction</h3>
<p>
		Images can be very complex and may contain many different points of interest -
	or more importantly, areas that are not interesting. As a result, tools meant
	for a specific usage may fail to perform adequately when applied on images with
	outside context. This project will take a domain and attempt to rectify this
	problem. <br>
		In specific, we want to be able to extract the license plate number from cars
	without including any superfluous information. Existing approaches exist, but
	they take advantage of the fact that the main sources of text are from license
	plates. See the figure below as an example from <a
	href='http://www.openalpr.com/cloud-api.html'>this website</a>.
</p>
	<br> <br>
	<h3>Teaser Figure</h3>
	<div class='row' style="text-align: center;">
	<img style="height: 325px;" alt="" src="ALPR-text.png">
	<img style="height: 325px;" alt="" src="teaser.png">
	</div>
	<br>

<br><br>
<!-- Approach -->
<h3>Approach</h3>
<p>
		Our approach is unique in the fact that we will combine different strategies to
	only extract information from license plates. This will lead to an application
	that can be used in more generalized models.</p>

    <p>We divide our work into two parts, the first part is image segmentation and
    the second part is OCR. The first part performs image segmentation on the image,
    and output corresponding segments, based on the methods we choose. After saving
    segments to a temporary directory, We pass these segments into an OCR model in
    order to retrieve license plate tag information.<br></p>

    <p>
    The idea behind segmenting this process into two areas is to reduce the
    burden of a single algorithm and to extract additional information from an
    image. For example, running solely OCR on an image may result in superfluous
    information. However, in conjunction with an image segmentation algorithm
    that specifically detects license plates, only the relevant portions of the
    images are processed.
    </p>

    <p>
    Because there exists a myriad of image segmentation algorithms, one of our
    goals was to see which one would preform well in this domain. As such, we
    chose the following algorithms:

    <h4>Image Segmentation</h4>

    <ul>
        <li>
            <h5>Graph Cut</h5>
            Graph cut was one of the algorithms that we learned about in class.
            Because graph cut attempts to segment the image based on a pixels
            surroundings, and the background of a license plate is monochromatic,
            then graph cut should be able to segment the license plate away from
            the rest of the image. 
        </li>
        <li>
            <h5>K-Means</h5>
            K-Means is another image segmentation algorithm, but is differs from
            graph cut in the sense that spatial proximity is not a factor in the
            segmentation. For example, if graph cut was unable to extract the
            license plate because the letters were a different color than the
            plate itself, k-means would be better suited to create a segment
            containing all the letters.
        </li>
        <li>
            <h5>Faster RCNN</h5>
            Lastly, we will attempt to use a neural network in order to segment
            the license plates. Recently, neural networks have been growing in
            popularity, and especially convolutional neural networks in the
            domain of computer vision. We choose to use the Faster RCNN to
            segment the images with pre-trained weights.
        </li>
    </ul>
    </p>

    <h4>Optical Character Recognition</h4>
    <p>
    As for optical character recognition, we chose to leverage an online API to
    prevent spending too much time developing an adequate OCR application. The
    API that we chose to use was OpenALPR. OpenALPR is an application that
    detects license plates from images. However, as seen in the introduction, it
    can be susceptible to false positives, finding text in areas that are not
    license plates.
    </p>

    <!--<p>		
    The first part to this specialized OCR is to separate the regions of the image
	that are of interest. This is accomplished using image segmentation. After
	identifying regions of interest, we can perform optical character recognition on
	these regions in order to obtain the license plate numbers. <br>
    </p> 


		There are many different implementations of both image segmentation as well as
	optical character recognition. As a result, we want to experiment with the
	different implementations and see which combinations result in accurate
	representations.
    </p> -->

<br>
<!-- Results -->
<h3>Experiments and results</h3>

<h4>
    Dataset
</h4>
<p>

		One requirement for this project is to collect a dataset. We can leverage 
	<a href='https://platerecognizer.com/number-plate-datasets/'>this dataset</a>,
	which is a collection of license plates. We will focus on US license plates.<br>
    This dataset includes annotations for license plates, so we can easily
    utilize this dataset to calculate accuracy for a large number of images. To
    calculate accuracy, we chose to take the number of matching predicted and
    ground truth annotations and divide that by the number of ground truth
    annotations and any additional false positives. This would penalize the
    algorithm for detecting text that is not part of a license plate.
</p>

<!-- Final Currently Here -->

<p>
	For the first part, we have utilized a k-means clustering algorithm[1] and a graph cut algorithm with GMM and min cut[2] to segment the image (the methods we have learned in lectures). See sample outputs for k-means below (right click and open the image in new tab to see the picture): 
</p>

<br>
<div class='row' style="text-align: center;">
<img style="width: 450px;" alt="" src="res/phillycar.png">
</div>
<div  style="text-align: center;"><b>Original Sample Image: <a href='https://whyy.org/articles/illegal-parkers-beware-philadelphia-wants-you-out-of-the-crosswalk-and-off-the-sidewalk/'> Philly Car </a></b></div>
<br>
<br> <br>
<div class='row' style="text-align: center;">
<img style="width: 225px;" alt="" src="res/kmeans/center 0.png">
<img style="width: 225px;" alt="" src="res/kmeans/center 2.png">
<img style="width: 225px;" alt="" src="res/kmeans/center 4.png">
<img style="width: 225px;" alt="" src="res/kmeans/center 6.png">
<img style="width: 225px;" alt="" src="res/kmeans/center 8.png">
</div>
<div  style="text-align: center;"><b>K-means clustering segmentation results</b></div>

<br>
<p>
	At first glance one could see this is potentially a good result for extracting image segments, because we can differentiate plate information. Also, since plate numbers are usually written in one consistent color, k-means can definitely be a way to approach the problem. However, k-means clustering heavily relies on the initial points that we choose. Even with the same cluster number (k=15 in the example above), each run can render different segmentation results. In some runs, the plate information is well decomposed and we cannot read it in any segments. Another problem with k-means clustering, due to how it works, is that we cannot filter out segments that are not license plates, so passing these into the OCR algorithm will either yield no results or jumbled results. <br>
</p>
<p>
	Another algorithm we tried for image segmentation is graph cut using GMM combined with min cut. See sample outputs below. The advantage of graph cut is that it can filter out segments without plates, so that the input image passed to OCR part is guaranteed to have plate information and thus not wasting running time. But this method, too, has its limitations. To use this algorithm, we have to manually select a rectangle area that contains useful information, i.e., foreground object. If algorithm doesn't filter out the plate, we can further provide a "mask" that highlights foreground objects and mask background objects. This method works well when we have only a handful of images to process. When we wanted to provide a bigger dataset for neural network models, it becomes tedious labor work. 
	<br><br>
</p>

<div class='row' style="text-align: center;">
<img style="width: 450px;" alt="" src="res/graphcut/rect_seg_philly.jpg">
</div>

<div  style="text-align: center;"><b>Graph cut segmentation result, using only rectangle area selection </b></div>
<br>

<div class='row' style="text-align: center;">
<img style="width: 450px;" alt="" src="res/graphcut/mask_seg_philly.jpg">
</div>

<div  style="text-align: center;"><b>SGraph cut segmentation result, using mask selection, specifically including plate area </b></div>
<br>


<br> <br>
<div class='row' style="text-align: center;">
<img style="width: 450px;" alt="" src="res/amsterdam.jpg">
</div>
<div  style="text-align: center;"><b>Original Sample Image: <a href='https://www.amsterdam.nl/en/parking/on-street-parking/'> Amsterdam Car  </a></b></div>
<br>

<div class='row' style="text-align: center;">
<img style="width: 450px;" alt="" src="res/graphcut/rect_seg_ams.jpg">
</div>

<div  style="text-align: center;"><b>Segmented (Extracted) image result, using only rectangle area selection</b></div>
<br>

<p>
	We are also using the OpenALPR API to extract the plates from the image segments. Right now the system is able to recognize plates from sample image segments. Note that if we decide to pass in just the segmented images for OCR, we may want to use a more general purpose OCR. 
		We will try multiple different image segmentation methods. Some will include
	in-class methods such as edge detection or texture-based detection. We will also
	leverage other methods including convolutional neural nets. Some models we will
	experiment with are <a href='https://arxiv.org/abs/1506.02640'>YOLO</a> and
	<a href='https://arxiv.org/abs/1506.01497'>Faster RCNN</a>. <br>
</p>
<p>
		As for OCR, we will use <a
	href='https://github.com/openalpr/openalpr'>OpenALPR</a>. This is an API that
	performs OCR on images to detect and digitalize license plate numbers. By
	passing regions of interest into the API, we should be able to extract the
	license plate numbers for each instance. Alternatively, we could use a simpler,
	more basic OCR such as <a
	href='https://arxiv.org/abs/1506.02640'>pytesseract</a> and see how performance
	differs - if at all.
	<br>
</p>
<p>
		For our machine learning image segmentaton, we will tend to use pre-trained models. The
	reason for this is that we do not have access to a particularly large dataset.
	However, in the case that we do train our own models, we will reserve 100 images
	for the test set, while all other images will be used for training. These 100
	images are held aside for hand-annotating and grading. <br>
</p>
<br>

<h3>Qualitative Results</h3>
<br>

<h3>Future Work</h3>
<p>
	Our immediate next step is to find a way to determine  whether a segment is a license plate or not, as currently k-means clustering seems to be a better approach than graph cut. In the future, we plan to experiment with more segmentation algorithms including edge detection with a Hough transform or convolutional neural networks like Faster-RCNN. We also want to research more OCR algorithms. In addition, being able to use confidence for both plate detection and plate OCR would allow us to give a confidence level for the final result of a license plate.
</p>
<br>
	
<p>
	Our work is included in this <a
    href='https://github.com/shuyteng/CS6476-Project'>Github Repo</a>.
</p>
<!--
Provide details about the experimental set up (number of images/videos, number
of datasets you experimented with, train/test split if you used machine learning
algorithms, etc.). Describe the evaluation metrics you used to evaluate how well
your approach is working. Include clear figures and tables, as well as
illustrative qualitative examples if appropriate. Be sure to include obvious
baselines to see if your approach is doing better than a naive approach (e.g.
for classification accuracy, how well would a classifier do that made random
decisions?). Also discuss any parameters of your algorithms, and tell us how you set the values of those parameters. You can also show us how the performance
varies as you change those parameter values. Be sure to discuss any trends you
see in your results, and explain why these trends make sense. Are the results as
expected? Why?
-->


<br><br>
<!-- References -->
<h3>References</h3>
1. <a href='https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_grabcut/py_grabcut.html'> Interactive Foreground Extraction using GrabCut Algorithm </a> <br>
2. <a href='https://towardsdatascience.com/introduction-to-image-segmentation-with-k-means-clustering-83fd0a9e2fc3'> Introduction to Image Segmentation with K-Means clustering </a> <br>
3. <a href='https://www.openalpr.com/cloud-api.html'> OpenALPR Cloud API </a><br>
4. <a href='https://arxiv.org/abs/1506.01497'>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a> <br>
5. <a href='https://platerecognizer.com/number-plate-datasets/'>Number Plate Datasets</a> <br>
6. <a href='https://arxiv.org/abs/1506.02640'>You Only Look Once: Unified, Real-Time Object Detection</a><br>

<br><br>

</div>
</div>

<br><br>

</body></html>
